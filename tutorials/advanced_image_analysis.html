<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>advanced_image_analysis</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>


</head>

<body>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h1 id="toc_0">Advanced Image Analysis</h1>

<p>Chengliang Tang, Yuting Ma</p>

<p>Applied Data Science </p>

<h2 id="toc_1">Scale Invariant Feature Transformation (SIFT)</h2>

<ul>
<li>Scale Invariant Feature Transformation (SIFT) was introduced by D.Lowe, University of British Columbia,in 2004. It is currently one of the most commonly used feature detector and descriptor for image analysis. It operates in 4 parts:

<ul>
<li>Detector

<ol>
<li>Find Scale-Space Extrema</li>
<li>Keypoint Localization &amp; Filtering

<ul>
<li>Improve keypoints and throw out bad ones</li>
</ul></li>
</ol></li>
<li>Descriptor

<ol>
<li>Orientation Assignment

<ul>
<li>Remove effects of rotation and scale</li>
</ul></li>
<li>Create descriptor

<ul>
<li>Using histograms of orientations</li>
</ul></li>
</ol></li>
</ul></li>
</ul>

<h3 id="toc_2">Feature Detection</h3>

<h4 id="toc_3">Scale-Space Extrema Detection</h4>

<ol>
<li>Construct a scale space

<ul>
<li>Increase in octave in a number of scale levels
<img src="./demo_img/octave.png" alt="Alt text"></li>
<li>Example
<img src="http://aishack.in/static/img/tut/sift-octaves.jpg" style="border:none;"></li>
</ul></li>
<li>Image is convolved with Gaussian filters at different scales

<ul>
<li>Under a variety of reasonable assumptions, the only possible scale-space kernel is the Gaussian function.</li>
<li><img src="http://chart.googleapis.com/chart?cht=tx&chl= L(x, y, k\sigma)" style="border:none;"> is the convolution of the original image <img src="http://chart.googleapis.com/chart?cht=tx&chl= I(x, y)" style="border:none;">with Guassian blur <img src="http://chart.googleapis.com/chart?cht=tx&chl= G(x, y, k\sigma)" style="border:none;"> --&gt; <img src="http://chart.googleapis.com/chart?cht=tx&chl= L(x, y, k\sigma) = G(x,y, \sigma)*I(x,y)
" style="border:none;"> </li>
</ul></li>
<li><p>The difference of successive Gaussian-blurred images are taken --&gt; Difference of Gaussian (DoG) </p>

<ul>
<li><img src="http://chart.googleapis.com/chart?cht=tx&chl= D(x, y, \sigma) = L(x, y, k_i \sigma) - L(x, y, k_j \sigma)" style="border:none;">= difference at different scales $k<em>i$ and $k</em>j$
<img src="./demo_img/scale_octave.png" alt="Alt text"></li>
</ul></li>
<li><p>Keypoints are then taken as <strong>maxima/minima</strong> of the Difference of Gaussians (DoG) that occur at multiple scales.</p></li>
</ol>

<p><img src="http://aishack.in/static/img/tut/sift-maxima-idea.jpg" style="border:none;"></p>

<ul>
<li>Parameter to tune:

<ul>
<li>Number of octaves</li>
<li>Number of scale levels</li>
<li>Initial $\sigma$</li>
<li>Initial $k$</li>
</ul></li>
</ul>

<h4 id="toc_4">Keypoint Localization</h4>

<ul>
<li>Scale-space extrema detection results in too many keypoints, some of which are unstable. </li>
<li>Keypoint localization used Taylor series expansion of scale space to get more accurate location of extrema

<ul>
<li>It removes detected keypoints with intensity lower than a <strong>contrastThreshold</strong></li>
<li>It removes detected keypoints that are consider to be at the edges.

<ul>
<li>It uses a 2x2 Hessian matrix (H) to compute the pricipal curvature. If a ratio between the squared trace and the determinant is less than a <strong>edgeThreshold</strong>, the keypoint is discarded. </li>
</ul></li>
</ul></li>
<li>Parameters:

<ul>
<li>contrast Threshold</li>
<li>edge Threshold</li>
</ul></li>
</ul>

<h3 id="toc_5">Feature Description</h3>

<p>After getting a set of &quot;good&quot; keypoints, we need to quantify the characteristics of the local image patch. </p>

<h4 id="toc_6">Orientation Assignment</h4>

<ul>
<li>An orientation is assigned to each keypoint to achieve invariance to image rotation.
<img src="./demo_img/orient.png" alt="Alt text"></li>
<li>A neigbourhood is taken around the keypoint location depending on the scale, and the gradient magnitude and direction is calculated in that region. 
<img src="http://aishack.in/static/img/tut/sift-orientation-eqns.jpg" style="border:none;">
(Computation details, see reference.)</li>
<li>An orientation histogram with 36 bins covering 360 degrees is created

<ul>
<li>It is weighted by gradient magnitude and gaussian-weighted circular window with $\sigma$ equal to 1.5 times the scale of keypoint. </li>
</ul></li>
<li><p>The highest peak in the histogram is taken and any peak above 80% of it is also considered to calculate the orientation. It creates keypoints with same location and scale, but different directions. So, orientation can split up one keypoint into multiple keypoints.
<img src="./demo_img/orient2.png" alt="Alt text"></p></li>
<li><p>It contribute to stability of matching.</p></li>
</ul>

<h4 id="toc_7">Keypoint Descriptor</h4>

<p>SIFT descriptor is a 128-dimenional vector, derived by:</p>

<ol>
<li>a 16x16 window around the keypoint. This 16x16 window is broken into sixteen 4x4 windows. 
<img src="http://aishack.in/static/img/tut/sift-fingerprint.jpg" style="border:none;"></li>
<li>Within each 4x4 window, gradient magnitudes and orientations are calculated. These orientations are put into an 8 bin histogram. <img src="http://aishack.in/static/img/tut/sift-4x4.jpg" style="border:none;"></li>
<li>Doing this for all 16 windows, you would&#39;ve &quot;compiled&quot; 16 totally random orientations into 8 predetermined bins. Once you have all 16*8 = 128 numbers, you normalize them into &quot;feature vector&quot;</li>
</ol>

<p><img src="./demo_img/sift.png" alt="Alt text"></p>

<h2 id="toc_8">Use OpenCV-Python for Image Analysis</h2>

<h3 id="toc_9">Use OpenCV-Python From R</h3>

<h4 id="toc_10">Installation</h4>

<ul>
<li>Install Python</li>
<li>Install OpenCV for Python

<ul>
<li><a href="http://www.pyimagesearch.com/2015/06/15/install-opencv-3-0-and-python-2-7-on-osx/">A Guide of install OpenCV 3.0 and Python 2.7+ on OSX</a></li>
<li>It is recommended that OpenCV is installed in a virtualenv</li>
</ul></li>
<li>Install R package &quot;rPython&quot;</li>
</ul>

<h4 id="toc_11">How to Use</h4>

<ol>
<li><p>Open terminal, activate (work on) the virtualenv created with OpenCV by <code>source myvirtualenv/bin/activate</code> or <code>workon myvirtualenv</code>.</p></li>
<li><p>Run R in the virtualenv and <code>library(rPython)</code></p></li>
<li><p>Write a sample Python code with <code>import cv2</code> to see if the module can be successfully imported. For instance: </p></li>
</ol>

<div><pre><code class="language-none"># test.py
import numpy as np
import cv2
img = cv2.imread(&#39;cat.jpg&#39;)
d = img.shape</code></pre></div>

<p>4.Try to call test.py from R and load variables from Python to R.</p>

<div><pre><code class="language-none">&gt; library(rPython)
&gt; #Load/run the main Python script
&gt; python.load(&quot;test.py&quot;)
&gt; # Get the variable from python
&gt; python.load(&quot;test.py&quot;)
&gt; d &lt;- python.get(&quot;d&quot;)
&gt; d
[1] 400 600   3</code></pre></div>

<ul>
<li>Note that the object loaded from Python to R need to be <strong>JSON serializable</strong>. JSON is a format that encodes objects in a string. Serialization means to convert an object into that string, and deserialization is its inverse operation. When transmitting data or storing them in a file, the data are required to be byte strings, but complex objects are seldom in this format. </li>
<li>The most commonly used data structure in Python is <code>ndarray</code> from numpy, which is analogous to n-dimensional array in R. However, <code>ndarray</code> is not JSON serializable so it cannot be directly loaded to R. To overcome this issue, you can convert <code>ndarray</code> to <code>list</code> in your Python script. For instance: by adding <code>x_list = x.tolist()</code> where <code>x</code> is a <code>ndarray</code>, it will be read as an list in R. </li>
</ul>

<h4 id="toc_12">Potential Problem and Solution</h4>

<ul>
<li>If you have different version of python

<ul>
<li>From terminal (before open R), eg.: <code>export RPYTHON_PYTHON_VERSION=2.7.11</code></li>
<li>always check your <code>Sys.getenv()</code> in R.</li>
</ul></li>
<li>When installing OpenCV, after step 7 you may need to restart the system to reconfigure the <code>.bash_profile</code> before proceeding to step 8.</li>
<li>You need to have opencv libraries ready for R to use. So if runs into library error, you may need to copy opencv DYLD libraries to R library: <code>cp -R ~/opencv/build/lib /Library/Frameworks/R.framework/Resources/lib</code></li>
<li>Unfortunately, I haven&#39;t figured out how to configure RStudio to the python virtualenv. RStudio links to the system Python path by default. Hope you can help me with it. </li>
<li>Unfortunately too, rPython is currently not available for Windows OS.

<ul>
<li>Solution:

<ul>
<li>Use some virtual machines like VMware or VirtualBox on windows and install Linux and carry out your work</li>
<li>Call python from R through Rcpp, which basically connect R and python via C++</li>
<li>Directly use python and use intermediate outputs to facilitate the data analysis.

<ul>
<li>http://gallery.rcpp.org/articles/rcpp-python/</li>
<li>Cons: lack of interactivity</li>
</ul></li>
</ul></li>
</ul></li>
</ul>

<h3 id="toc_13">Use SIFT in OpenCV</h3>

<div><pre><code class="language-none"># test_sift.py
import numpy as np
import cv2
img = cv2.imread(&#39;cat.jpg&#39;)
gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
sift = cv2.xfeatures2d.SIFT_create()
(kps, descs) = sift.detectAndCompute(gray, None)
print(&quot;# kps: {}, descriptors: {}&quot;.format(len(kps), descs.shape))
img=cv2.drawKeypoints(gray,kps, img)
cv2.imwrite(&#39;sift_keypoints.jpg&#39;,img)
descs_list = descs.tolist()</code></pre></div>

<ul>
<li>The output image with key points detected
<img src="./demo_img/sift_keypoints.jpg" alt="Alt text"></li>
</ul>

<p>In R:</p>

<div><pre><code class="language-none">&gt; python.load(&quot;test_sift.py&quot;)
&gt; descs_list &lt;- python.get(&quot;descs_list&quot;)
&gt; # Convert list to array
&gt; img_descs &lt;- do.call()
&gt; length(img_descs)
[1] 305
&gt; img_sift &lt;- do.call(&quot;rbind&quot;, img_descs)
&gt; dim(img_sift)
[1] 305 128
&gt; img_sift[1:5, 1:5]
     [,1] [,2] [,3] [,4] [,5]
[1,]   56   10    8   13   16
[2,]    3   29   26   35   15
[3,]    0    0    2   36   83
[4,]   71   42    6    1    2
[5,]    0    0    1   16  115</code></pre></div>

<h3 id="toc_14">Other Methods in OpenCv</h3>

<h4 id="toc_15">Harris Corner Detection</h4>

<div><pre><code class="language-none"># test_corner.py
import cv2
import numpy as np
filename = &#39;cat.jpg&#39;
img = cv2.imread(filename)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
gray = np.float32(gray)
dst = cv2.cornerHarris(gray,2,3,0.04)
#result is dilated for marking the corners, not important
dst = cv2.dilate(dst,None)
# Threshold for an optimal value, it may vary depending on the image.
img[dst&gt;0.01*dst.max()]=[0,0,255]
cv2.imshow(&#39;dst&#39;,img)
if cv2.waitKey(0) &amp; 0xff == 27:
    cv2.destroyAllWindows()</code></pre></div>

<p>The detected points are:
<img src="./demo_img/corner.png" alt="Alt text"></p>

<p>And a lot more to be explored. </p>

<h2 id="toc_16">Use Matlab For Image Analysis</h2>

<h3 id="toc_17">Use SIFT in Matlab</h3>

<p>SIFT and other advanced image analysis tools are available in Matlab from library <a href="http://www.vlfeat.org/">VlFeat</a>. The <a href="http://www.vlfeat.org/applications/caltech-101-code.html">sample code</a> of implementing SIFT in matlab is also avaiable.</p>

<h3 id="toc_18">Other Methods</h3>

<p>In addition to the ordinary SIFT, VlFeat also offers some other useful tools for feature extraction and description, which might be more efficient computationally. For instance:</p>

<ul>
<li><a href="http://www.vlfeat.org/overview/mser.html">MSER feature detector</a><br></li>
<li><a href="http://www.vlfeat.org/api/dsift.html">Dense SIFT</a></li>
<li><a href="http://www.vlfeat.org/overview/hog.html">HOG features</a></li>
</ul>

<h2 id="toc_19">Bag-of-Words (BoW) Model</h2>

<ul>
<li>In natural language processing, a text corpus, such as a sentence or a document, is represented as an unordered &quot;bag&quot; of individual words, regardless of grammar and word ordering.</li>
<li>The bag-of-words model is commonly used in methods of document classification, where the (frequency of) occurrence of each word is used as a feature for training a classifier.</li>
</ul>

<h3 id="toc_20">Visual Bag-of-Words Representation</h3>

<ul>
<li>An image can be treated as a document</li>
<li>Local features extracted from the images are considered as the &quot;visual words&quot;</li>
<li>An image can be represented as an unordered collection of visual words, such as histogram. 
<img src="https://gilscvblog.files.wordpress.com/2013/08/figure21.jpg?w=625" style="border:none;", width = 500>
<img src="https://gilscvblog.files.wordpress.com/2013/08/figure31.jpg" style="border:none;", width = 500></li>
</ul>

<h3 id="toc_21">Pipeline</h3>

<ul>
<li>Feature extraction (SIFT)

<ul>
<li>Feature Detection</li>
<li>Feature Description</li>
</ul></li>
<li>Visual vocabulary (codebook) construction</li>
<li>Image representation based on learned vocabulary</li>
<li>Learning and Recognition
<img src="https://raw.githubusercontent.com/ChengliangTang/Temp-Images-for-2017-Spring/master/Screenshot_4.png" style="border:none;"></li>
</ul>

<h2 id="toc_22">References</h2>

<ul>
<li><a href="http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html">OpenCV 3.0 official doumentation</a></li>
<li><a href="http://www.inf.fu-berlin.de/lehre/SS09/CV/uebungen/uebung09/SIFT.pdf">A Nice SIFT Tutorial</a></li>
<li><a href="http://www.vlfeat.org/index.html">VlFeat</a></li>
</ul>




</body>

</html>
